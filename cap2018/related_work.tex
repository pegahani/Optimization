%%================================================================
%\textbf{Literature Review.}  Typically, in MDPs the reward functions are estimated either from observations or external sources. Mannor et al. \shortcite{Mannor2007} demonstrate that the policy found via an optimisation process under the MDPs with numerical parameters, sometimes can be much worse than the anticipated policy. This motivates using MDPs that account for this ambiguity in model parameters. To cope with this problem, first the \textit{MDP with imprecise rewards (IRMDP)} should be modelled mathematically. There exists several models in the literature including symbolic-based rewards approaches \cite{Furnkranz2012,Weng2012} and numerical-based rewards approaches \cite{bell1982,Regan2009,Xu2009}. We focus on group of approaches taking decision-theoretic point of view, and consider a set of MDPs with different reward functions --modeling under uncertainty-- namely \textit{Imprecise Reward MPDs (IRMDPs)}. 


%In order to find the optimal policy under uncertainty, robust solutions typically provides guaranties on the worst case performance. 
One common approach in computing robust solution is the \textit{maximin} method, that computes a policy maximising the value with respect to the worst-case scenario \cite{GIVAN2000,Iyengar2005,mastin2012,Nilim2005}. Minimax robustness can be considered as a game between two adversaries, one finds a policy with maximum values while the adversary chooses an instantiation of the reward functions that minimise the expected value. There are some recent works that propose some techniques for dependent uncertainties in MDPs \cite{Mannor2012,Wiesemann2013}. In this paper, the uncertain reward functions are independent from each other. 

\textit{maximin} policies are conservative naturally \cite{Delage2007}, thus \textit{minimax regret} approach \cite{Regan2009,Xu2009} has been introduced as an approach to cope with this issue. %In \textit{minimax} regret criterion the goal is to find a policy that has the least value of maximum regret over all instantiations of rewards. 
Several methods in the past \cite{Ahmed2017,Regan2009,Regan2010,Xu2009} have only focused on computing  \textit{optimal stochastic policies} for IRMDPs. %, i.e. a policy proposing several actions associated with probabilities for a state in IRMDP. 
To the best of our knowledge, there is no work that handle deterministic policy computation on MDPs under uncertainties. The existing works on deterministic policies computation deal usually with MDPs with precise parameters \cite{Dolgov2005,Montufar2015}. %In this paper we introduce an approach for computing the closest deterministic policy with the minimum value on maximum regret over all instantiations of rewards. 