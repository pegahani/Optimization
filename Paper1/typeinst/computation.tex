\section{Experimental results}\label{sec:experiments}

In this section, we provide an experimental evaluation of our approach
based on two classes of test instances. More precisely, we test our approach on the following MDPS:
\begin{itemize}
\item Random MDPs (\texttt{Random}).
\item Diamond MDPs (\texttt{Diamond}).
%\item Grid MDPs (\texttt{Grid}).
\end{itemize}


For each class of MDP, we provide a brief description of its structure and of the parameters used to generate the testbed.

As already mentioned in the introduction, the aim of this section is two-fold:

(i) First of all, we would like to motivate the use of deterministic over stochastic policies. To do this, we compare the value of the maximum regret obtained by the optimal deterministic policy with the one of the rounding policy explained in Section~\ref{sec:rounding}. 
%
%The TUP uses the concept of sparsity presented in this paper to generalize the methodology proposed in~\cite{Buchheim18} to solve efficiently combinatorial problems with a quadratic objective function.
%In Section~\ref{sec:TUP} we show how the application of TUD allows to improve the dual bounds obtained in the approach presented in~\cite{Buchheim18}.  

(ii) Secondly, we would like to assess the additional computational effort of computing the optimal deterministic policy in comparison to the one needed to compute the optimal stochastic policy. 	 

In the following section we use the notion of Time ratio ($TR$) and Value Ratio ($VR$), defined as follows:

\begin{definition}
For a given MDP, let $MR(f^{\hat{\pi}}, \mathcal{R})$ be the maximum regret of the rounding deterministic policy and $MR(f^{\pi^*}, \mathcal{R})$  be the maximum regret of the optimal deterministic policy. We have that the Value Ratio of such MDPs is defined as follows:
\begin{align}
VR = \dfrac{MR(f^{\hat{\pi}}, \mathcal{R})}{MR(f^{\pi^*}, \mathcal{R})}\;.
\end{align} 
Moreover, let $\hat{T}$ (respectively $T^*$) be the computing time necessary to calculate the rounding (respectively optimal) deterministic policy, we have that the Time Ratio is defined as:
\begin{align}
TR=\dfrac{T^*}{\hat{T}}\;.
\end{align}
 
\end{definition}

\subsection{Random MDPs}
\paragraph{Description}
A random MDP is defined by several parameters including its number of states $n$ , its number of actions $k$. All rewards are bounded between randomly two real numerical points. Transition function has several properties: from any state $s$ restrict transitions to reach $\lceil \log_2(n) \rceil$ states. For each pair of $(s, a)$ draw reachable states based on uniform
distribution over the set of states. For drawn states, transition probabilities are formed based on Gaussian distribution.
The initial state distribution Î² is uniform and we choose discount factor $\gamma = 0.95$. 
\paragraph{Analysis of the results}
In Table~\ref{tab:random} we present the results concerning the performances of our algorithm on random instances with $|S| \in \{5,10,15\}$ and $|A| \in \{1, 2, 3, 4, 5, 10\}$. For each combination of states and actions, we provide the average results over $10$ different instances. The first two columns report the Value Ratio and the Time Ratio (\texttt{VR} and \texttt{TR}). The column \texttt{\% diff} shows the percentage of cases where the optimal policy is different from the rounding policy. The final two columns shows the computing time of the baseline branch-and-bound algorithm presented in Section TODO and the improved version (cut-and-branch) presented in Section TODO.

We notice that $70\%$ of the times the optimal deterministic policy differs from the rounding and that the maximum regret of the rounding deterministic policy is $6\%$ worse than the maximum regret of the optimal deterministic policy. This moderate gap is probably due to the fact that random MDPs do not present a special structure. For such instances it is more unlikely to have extreme configurations like the one showed in Section~\ref{sec:comparison}. From the point of view of the computing times, calculating the optimal determinstic policy is one order of magnitude slower than computing the optimal stochastic policy. On the other hand, it is interesting to notice that the improved version lf the algorithm is two times faster than the basic version.  
Finally it is interesting to notice that even in the basic implementation of the algorithm, the number of additional Benders cuts added during the branch-and-bound is significantly low (on average no more than $2$ cuts per node). This explains why the cut-and-branch version of the branch-and-bound algorithm performs better than the standard version.


  

\input{tab_random.tex}


\subsection{Diamond MDPs}
\paragraph{Description}
This class of MDPs has been introduced for the first time in~\cite{benavent2018}. 
In this family of problems the reward of a few states suffices to generate a lot of uncertainty about the optimal policy. This class of instances is an interesting set of instancess to test our proposed algorithm.
This class of MDPs has a diamond structure, with one top and one bottom state (playing the role of source and sink of the  MDP), one intermediate layer of states, containing all the uncertainty in the reward, plus two intermediate layer between the extrem states and the intermediate layer.
In Figure~\ref{fig:diamond}, the structure of the diamond MDPs is presented.

\begin{figure}[h]
\begin{center}
\includegraphics[width=8cm]{images/diamond.png}
\end{center}
\caption{Diamond MDP: actions $a_0$ (left) and $a_1, a_2$ (right).}
\label{fig:diamond}
\end{figure}

From a point of viex of the parameters, the authors proposed to have that action $a_0$ would reach each child with a probability of $0.5$. ON the other hand $a_1$ (resp. $a_2$) a probability of $p= 0.3$ (resp. $1-p$) to reach the left (resp. right) child node and to reach its parent otherwise.
The imprecise values of the rewards for the middle layer are $[-600,600]$, while the one of the bottom node is $[600,1000]$.

We propose a generalization of this family of MDP in two senses: we test a range of parameters for the probability $p \in \{5,10,\dots,40,45\}$ and we introduce also an additional intermediate layer, between the extreme states and the middle layer. In this way we have, in addition to $10$-states MDPs (callend one-level diamond MDPs) also  22-states MDPs (called two-level diamond MDPs). 

\paragraph{Analysis of the results}
In Table~\ref{tab:diamond} We show how the Time Ratio and the Value ratio change with the increase of p.
It is clear that for Diamond MDPs the situation is different than for Grid MDPs. In thi case, the heuristic deterministic policy is $20\%$ worse than the optimal deterministic policy and the computing time for the deterministic policy is less than one order of magnitude of the computing time of the rounding heuristic.


\begin{table}[h]																	
 \centering
 \small
 \setlength{\tabcolsep}{4.0pt}
 \renewcommand \arraystretch{1.8}
\begin{tabular}{ccccccccccc}																					
\texttt{p}	&	5	&	10	&	15	&	20	&	25	&	30	&	35	&	40	&	45	&	Avg.	\\	
\cmidrule(lr){1-1} \cmidrule(lr){2-10} \cmidrule(lr){11-11}
\texttt{VR} &	1.66	&	1.24	&	1.16	&	1.13	&	1.15	&	1.15	&	1.15	&	1.14	&	1.16	&	\textbf{1.22}	\\	
\texttt{TR} &	10.23	&	7.44	&	6.32	&	6.48	&	7.67	&	5.93	&	7.62	&	10.46	&	13.80	&	\textbf{8.44}	\\	\\
\end{tabular}
\caption{Time Ratio and Value Ratio for \texttt{Diamond}.}														\label{tab:diamond}								
\end{table}																						



%\subsection{Grid MDPs}
%blablabla
%\paragraph{Description}
%blablabla
%\paragraph{Analysis of the results}
%blablabla



\paragraph{Analysis of the Diamond MDPs}

