
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\usepackage{named}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[numbers]{natbib}
\usepackage{mathtools}

\usepackage{tabularx,ragged2e}
\newcolumntype{C}{>{\Centering\arraybackslash}X} % centered "X" column

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{tikz}
\usepackage{tikzsymbols}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\up}[1]{\textsuperscript{#1}}
\newcommand{\imp}[1]{{\color{red}{#1}}}
\newcommand{\PA}[1]{{\color{green}{#1}}}
\newcommand{\tfidf}{\emph{tf-idf}}
\newcommand{\NMF}{\emph{NMF}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newlength{\mylengthleft}
\setlength{\mylengthleft}{1ex}
\newlength{\mylength}
\setlength{\mylength}{\textwidth}
\addtolength{\mylength}{-\mylengthleft}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{xcolor}
\usepackage{url}

\urldef{\mailsa}\path|pegah.alizadeh@unicaen.fr|
\urldef{\mailsb}\path|{emiliano.traversi, ao}@lipn.univ-paris13.fr|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}
\newcommand{\ea}[1]{#1 \emph{et al.}}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Deterministic Solutions Based on Maximum Regrets in Markov Decision Processes with Imprecise Rewards}

% a short form should be given in case it is too long for the running head
\titlerunning{Deterministic Solutions for IRMDPs}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%

\author{Pegah Alizadeh \up{1} \and Emiliano Traversi \up{2} \and Aomar Osmani\up{2}}

%
\authorrunning{Pegah Alizadeh, Emiliano Traversi and Aomar Osmani}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published

\institute{(1) GREYC, UMR 6072, UNICAEN/CNRS/ENSICAEN, 14000 CAEN, FRANCE\\
(2) LIPN-UMR CNRS 7030, PRES Sorbonne Paris-cit\'e, FRANCE\\
\mailsa\\
\mailsb \\
%\url{http://www.springer.com/lncs}
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
In several real world situations a stochastic policy is not easily interpretable for system users, this might be due to the nature of the problem or to the system requirements. In these contexts, it is inevitable to provide a deterministic policy to the user. In this paper, we propose an approach for computing a deterministic policy in reasonable computational overhead in comparison to the required time for computing the optimal stochastic policy. To better motivate the use of an exact procedure for finding a deterministic policy, we show some cases where the intuitive idea of using a deterministic policy obtained after ``determinising" (rounding) the optimal stochastic policy leads to a deterministic policy different from the optimal.

\keywords{Markov Decision Process, Minimax Regret, Unknown Rewards, Deterministic optimal Policy, Stochastic Optimal Policy}
\end{abstract}


\section{Introduction}

%outline for introduction
%\begin{itemize}
%\item MDP are important to model many things
%\item examples of MDP (special attention to MDP that can would prefer to have a deterministic policy)
%\item introduce concept of unknown reward
%\item mention several ways to deal with unknown reward, among them minimax regret.
%\item but the majority of the approaches proposes a stochastic policy
%\item det policy are preferable in several situation
%-- ethic \\
%-- finance \\
%\end{itemize}

Markov Decision Processes (MDPs) have proven to be effective models for representing and solving sequential decision problems under uncertainty. It is natural to model a decision making problems in stochastic environments as MDPs such as navigation, robotics or service composition problems. In navigation context such as assistant autonomous vehicles, at each stage, the agent executes an action with probabilistic effects and this state conducts her to a next state and yields a reward (punishment). The goal is to maximise (minimise) the expected sum of rewards. The environment dynamic (traffics and roads) are modelled as states and actions with probability affections. Despite of knowing the final goal, specifying rewards or punishments for choosing actions in states is not obvious all the time %for several reasons
%Specifying exact rewards (punishments) is not obvious 
for multiple reasons:
\begin{itemize}
\item insufficient data to estimate rewards,
\item parts of models are too complex to detail: in assistant vehicle example defining exact rewards for all actions is time consuming or complicated and can variate during the driving process. For this reason, we consider them bounded in real valued intervals.
\item in communication with system's users: If the model is designed for different drivers with various preferences, even after limited number of communications with drivers and diminishing the unknown rewards to smaller sets, the MDP is still partially known. 
\end{itemize}

%In MDPs with unknown rewards, the system have all information about the dynamics (road and traffics) and final goal but it lacks some information about the user preferences inside the system
In order to find the best \textit{policy} (strategy) regarding the imprecise information on rewards, there are some robust optimization approaches in literature\cite{Ahmed2017,Iyengar2005,Nilim2003,Xu2009}. In this work, we focus on \textit{minimax regret} method \cite{Regan2009} in which proposes the optimal policy with the minimum lost in comparison with other possible policies. Majority of methods for computing the optimal policy including the exact or approximate ones propose a \textit{stochastic policy} for the MDP (see related work). That causes some problems for some real environments such as ethic cases, financial applications (Bitcoin and Crypto-currency) or any applications that can not risk more than once executions. In assistant autonomous vehicle case, if there is a risk of dealing an ethic dilemma \footnote{such as the famous train example. The conductor should decide between killing one persons or one person}, the optimal policy should be deterministic without putting the user in a situation to train between a north direction with $0.3$ or the west direction with $0.7$ probability by being in a special position.


In this paper we introduce a first in depth study of deterministic policy for minimax regret approach. Our method finds the best deterministic policy enough close to the stochastic solution calculated by the minimax regret approach. Some may claim that \textit{determinising} the stochastic policy computed by minimax regret is a \textit{deterministic optimal policy}. We present an MDP with imprecise rewards, as a counter example to this claim. We finally report on an experimental study on random MDPs with different structures, in which we compare deterministic and stochastic policies with different scenarios. 


%%================================================================
\section{Related Work}


%%================================================================

%This section concerns the required components of MDPs and minimax regret criterion for MDPs with imprecise rewards.

\section{Preliminaries}\label{sec:Preliminaries}
\textit{Markov Decision Process (MDP)} \citep{Puterman1994} is defined by a tuple $M(S, A, P, r, \gamma, \beta)$, where: $S$ is a finite set of states; $A$ is finite set of actions, $P: S \times A \times S \longrightarrow [0,1]$ is a \textit{transition function} where $P(s'|s,a)$ encodes the probability of going to state $s'$ by being in state $s$, and choosing action $a$; $r: S \times A \longrightarrow \mathbb{R}$ is a \textit{reward function} (or penalty, if negative) obtained by choosing action $a$ in state $s$; $\gamma \in [0, 1[$ is the discount factor; and $\beta: S \longrightarrow [0,1]$ is an \textit{initial state distribution function} indicating probability of initiating in state $s$ by $\beta(s)$.

A (stationary) \textit{deterministic policy} is a function $\pi: S \longrightarrow A$, which prescribes to take action $\pi(s)$ when in state $s$. A (stationary) \textit{stochastic policy} is a function $\tilde{\pi}: S \times A \longrightarrow [0,1]$ which indicates with probability $\tilde{\pi} (s,a)$, action $a$ is chosen in state $s$ according to policy $\tilde{\pi}$. A policy $\pi$ induces a \textit{visitation frequency function} $f^{\tilde{\pi}}$ where $f^{\tilde{\pi}}(s,a)$ is the total discounted joint probability of being in state $s$ and choosing action $a$ (see Section $6.9$ in \cite{Puterman1994}):
\begin{align*}
f^{\tilde{\pi}}(s, a) = \sum_{s' \in S} \beta(s') \sum_{t=0}^{\infty} \gamma^{t-1}(S_t = s', A_t = a | S_1 = s)
\end{align*}
where the sum is taken over trajectories defined by $S_0 \sim \beta, A_t \sim \tilde{\pi}(S_t)$ and $S_{t+1} \sim P(.|S_t,A_t)$. The policy is computable from $f^{\tilde{\pi}}$, via 
\begin{align}\label{pi_f}
\tilde{\pi}(s,a) = \frac{f^{\tilde{\pi}}(s, a)}{\sum_{a'} f^{\tilde{\pi}} (s,a')}\;.
\end{align}
For a deterministic policies we have that $f^{\pi}(s,a)= 0$, $\forall a \neq \pi(s)$.\\
Policies are evaluated by expectation of discounted sum of rewards w.r.t to the infinite horizon discounted criterion, namely \textit{value function} $V: S \longrightarrow \mathbb{R}$: 
$V^{\tilde{\pi}}(s) = \mathbb{E}(\sum_{t=0}^{\infty} \gamma^{t}$ $r(s_t, \tilde{\pi}(s_t))$. %Taking into account the initial distribution $\beta$, each policy has an expected value function equal:
%\begin{align*}
%\mathbb{E}_{\sim \beta}[V^{\tilde{\pi}}(s)]=  \sum_{s \in S} \beta(s)V^{\tilde{\pi}}(s) = \beta \cdot V^{\tilde{\pi}}
%\end{align*}
Another way for defining the quality of policies is the \textit{Q-value function}   $Q: S \times A \longrightarrow \mathbb{R}$ given by:
\begin{align}\label{q-v}
Q^{\tilde{\pi}}(s, a) = r(s, a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\tilde{\pi}}(s')\;.
\end{align}

For a give initial state $\beta$, the value of the optimal policy is $\beta \cdot V^{\tilde{\pi}}$, this quantity can be expressed in terms of the visitation frequency function (see \cite{Puterman1994}): 
%The visitation frequency function and value function can be exchanged to each other:
\begin{align}\label{f-v}
%(\sum_{s \in S} \beta(s)V^{\tilde{\pi}}(s)) \;\; \;\;  
\beta \cdot V^{\tilde{\pi}} = r \cdot f^{\tilde{\pi}}\;.
%\;\;\;\;  (\sum_{s \in S} \sum_{a \in A} r(s,a) f^{\tilde{\pi}}(s,a) )
\end{align}
An MDP always has an optimal policy $\pi^*$ such that; $\pi^* = \text{argmax}_{\pi} \beta \cdot V^{\pi}$ or $f^{*} = \text{argmax}_{f} r \cdot f$ where the optimal policy can be recovered from $f^*$ using Equation \ref{pi_f}. 
%%================================================================

\subsection{MDPs with Uncertain Rewards}
When designing real cases as MDPs, specifying the reward function is generally a hard problem. For instance preferences stating which (state, action) pairs are good or bad should be interpreted into numerical costs. Note that even knowing all these preferences is time consuming. In order to
tackle this complexity, we use an MDP with \textit{imprecise reward values (IRMDP)}. An IRMDP \citep{Regan2009} is a tuple $\tilde{M}(S, A, P, \tilde{r}, \gamma, \beta)$ where $S, A, P, \gamma$ and $\beta$ are the same as MDP while $\tilde{r}$ is a set of possible reward functions on $S \times A$. $\tilde{r}$ models the uncertainty on real reward values. To stay coherent, we use the notation presented by \shortcite{benavent2018}, i.e. $M, \tilde{M}, r, \tilde{r}$ signify the standard MDP, IRMDP, real valued reward function and uncertain reward function model respectively.  

Similar to several previous works in the literature \cite{Ahmed2017,alizadeh2015,benavent2018,Regan2009,Weng2013}, we assume that $\tilde{r}$ is given as a polytope. For each $r(s,a) \in \tilde{r}$, the reward uncertainty is restricted in an interval. Thus $\tilde{R}$ is modelled as polytope $C \cdot \overrightarrow{r} \leq \overrightarrow{d}$ where $C$ is $k \times |S||A|$ dimension matrix, $\overrightarrow{d}$ is a $k$ dimensional column vector and $\overrightarrow{r} = (r(s_0,a_0), r(s_0,a_1), \cdots, r(s_0,a_{|A|}), \cdots, r( s_{|S|},a_0), r(s_{|S|},a_1), \cdots, r(s_{|S|},a_{|A|}) )$
 

\subsection{Minimax Regret}\label{sec:minimax-regret}
In order to solve the IRMDP $\tilde{M}$ with imprecise rewards bounded in the $\mathcal{R}$ polytope, we use the \textit{minimax regret criterion} method applied in some sequential-decision problems \cite{Regan2009,Xu2009}. Minimax regret is a robust optimization method in presence of uncertain data for approximating optimal policies. 

\begin{definition}
The \textbf{regret} of policy $f^{\pi}$ (has an equivalent policy $\pi$ according to Equation \ref{pi_f}) over reward function $r \in \mathcal{R}$ is defined as the loss or difference in value between f and the optimal policy under $r$: $R(f^{\pi}, r) = \text{max}_{g} \; r \cdot g - r \cdot f$
\end{definition}

\begin{definition}
The \textbf{maximum regret} for policy $f^{\pi}$ is the maximum regret of this policy w.r.t the reward set $\mathcal{R}$: $MR(f^{\pi}, \mathcal{R}) = \text{max}_{r \in \mathcal{R}}\;R(f^{\pi},r)$ 
\end{definition}

In other words, when we should select the $f$ policy, what is the worst case lost over all possible rewards $\mathcal{R}$. Considering it as a game, the adversary tries to find a reward value in order to maximise our lost.  

\begin{definition}
\textbf{minimax regret} of feasible reward set $\mathcal{R}$ is defined as:\\ $MM(\mathcal{R}) = \text{min}_{f^{\pi}}\; MR(f^{\pi}, r)$
\end{definition}

Any policy $f^*$ (its equivalent $\pi^*$) that maximises the minimum regret is the \textit{minmax-regret optimal policy} for the $\tilde{M}$. There are several approaches for computing the minimax regret \cite{alizadeh2015,benavent2018,Regan2009,daSilva2011,Xu2009}. In this paper we use the approach presented by Regan and Boutilier \citep{Regan2009} namely \textit{Benders Decomposition} \cite{Benders1962} to approximate the optimal minimimax regret policy. The idea is to formulate the problem as series of linear programs (LPs) and Mixed Integer Linear Programs (MILPs):

%----------------- minmax regret model --------------------

\begin{center}\label{minimax}
%%%%%
\texttt{Master Program}
%%%%%%
\begin{alignat}{3}
&\text{minimize}_{\delta, f} && \delta & \\
&\text{subject to:}&\quad& r\cdot g - r \cdot f \leq \delta \quad \forall \langle g_r, r \rangle \in \text{GEN}\label{delta_cut}\\
&& \quad& \gamma E^{\top} f + \beta = 0 
\end{alignat}
%%%%%%%
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center} 
%%%%%%%%
\texttt{Slave Program}
\begin{alignat}{3}
&\text{maximize}_{Q, V, I, r} && \beta \cdot V - r \cdot f \\
&\text{subject to:} &\quad& Q_a = r_a + \gamma P_aV &\quad \forall a \in A\\
&& \quad& V \geq Q_a  &\quad \forall a \in A\\
&& \quad& V \leq (1-I_a)M_a + Q_a  &\quad \forall a \in A\\
&& \quad& Cr \leq d \\
&& \quad& \sum_{a \in A} I_a = 1  \label{eq:sum_I}\\
&& \quad& I_a(s) \in \{0, 1 \} &\quad \forall s \in S, \; a \in A \label{eq:bin_I}\\
&& \quad& M_a = M^{\top} - M_a^{\perp} &\quad \forall a \in A
\end{alignat}
%%%%%%
\end{center}
%----------------- minmax regret model --------------------

The master program is a linear program computing the minimum regret with respect to all the possible combinations of rewards and adversary policies. We call GEN the set containg all the combinations of rewards and adversary policies. 
In the first set of constraints, one constraints for each element of GEN $\langle g_r, r \rangle \in \text{GEN}$ is considered. 
%These are pairs $\langle$ policy, reward $\rangle$s wining against policy $f$. 
The second set of constraints of the master problem, $\gamma E ^{\top}f+ \beta = 0$ guaranties that $f$ is a valid visitation frequency function. For the sake of abbreviation, $E$ matrix is generated according to the transition function $P$; $E$ is a $|S||A| \times |S|$-matrix with a row for each state action, and one column for each state:
\[   
E_{sa,s'} = 
     \begin{cases}
       P(s'|s, a) &\quad \text{if } s' \neq s\\
       P(s'|s, a) - \frac{1}{\gamma} &\quad \text{if } s' = s
     \end{cases}
\] 

The intuition behind this constraint is related to the dual linear program of the Bellman Equation (see for example \cite{Sutton1998}, Chapter $4$ or \citep{Puterman1994}, Section $6.9$). 

From a practical point of view, it is uncovenient to enumerate a priori all the constraints~\eqref{delta_cut}. Benders decomposition is base on the idea of starting with a small (maybe empty) subset of constraints~\eqref{delta_cut} and interact with the slave problem to have either a certificate of optimality of the master problem or a new inequality that ca potentially change the value of the master.

The slave program receives a feasible policy $f^*$ and a minimax regret value $\delta^*$; then it searches for a policy and a reward value with the max regret regarding to the given policy $f$, in other words, it finds a $\bar{r}$ and $\bar{g}$ such that $\bar{r} \cdot \bar{g}  - \bar{r}  \cdot f^* > \delta^*$. If such a $(\bar{r},\bar{g})$ is found, it is added to GEN and the master problem is solved again. If this is not the case, the procedure stops and $f^*$ is the (stochastic) policy that minimizes the maximum regret. 

The interaction between master and slave programs can be viewed as a game between two players. The master programs finds an optimal policy minimising regrets w.r.t the given adversaries found so far by the slave program, while the slave program searches for an adversary with the maximum gain against the master policy proposition $f$. This game continues until the slave problem can not find neither a policy as $g$ nor a reward as $r$ to generate a higher regret for the given $f$ by the master program.  

The slave problem is a reformulation of the MR($f, \mathcal{R}$) for the received policy $f$ from the master program. According to Equation \ref{f-v}, $r \cdot g - r \cdot f$ can be rewritten as $\beta \cdot V - r \cdot f$. Thus instead of finding the visitation frequency function as $g$, the value function $V$ related to the adversary policy should be computed. Among the presented constraints for the slave program, constraint $(5)$ ensures that $\forall s \quad Q(s, a) = r(s,a) + \gamma \sum_{s'} P(s'|s,a)V(s')$ (see Equation \ref{q-v}). On the other hand constraints $8$ and $9$ guaranty that $\forall s \quad Q(s, a) = V(s)$ and only one action $a$; $\forall a \quad M_a$ is sufficiently large to tight an upper bound for this equality. $M^{\top}$ is value of the optimal policy for maximum reward values i.e. in our case we compute the optimal policy for $\tilde{M}(S, A, P, r_{\text{max}}, \gamma, \beta)$ where $r_{\text{max}}(s, a) = r_l \quad \text{if} \; r_l \leq r(s, a) \leq r_u$; this can be found using the classical methods such as value iteration or policy iteration \cite{Sutton1998}. Similarly $M^{\perp}$ is the Q-value for the optimal policy with the worst rewards on $\mathcal{R}$. 

Regarding to constraints~\eqref{eq:sum_I} and~\eqref{eq:bin_I} $I$ is a $|S|\times|A|$-matrix defining the policy related to $V$. This policy is a deterministic policy with one and only one selected action $a$ per state $s$. Notice that the slave program proposes a deterministic adversary to the master program while the master program always approximate a stochastic policy. Since the adversary policy proposes an extreme policy w.r.t the given $f$, a MILP model for the slave program is sufficient.   

%%================================================================

\section{An exact Enumerative scheme to find the optimal deterministic solution} 

To find the optimal deteministic solution,  we incorporate the Benders decomposition procedure described in the previous section in a branch-and-bound framework (see for example \cite{bertsimas2005optimization}, Section 11). 

A branch-and-bound algorithm consists of a clever enumeration of the space of feasibile policies through a space search: the set of deterministic policies that can potentially be the optimal is represented with a rooted tree with the full set at the root. The algorithm explores branches of this tree, where each branch represents a subset of the solution set.
Once a new branch of the tree is created, and before that branches is splitted again in additional subbranches, a (lower) bounding procedure is executed on that branch. The bounding procedure gives an underestimation of the optimal solution of the problem over the feasible set associated to the given branch.
The branch is hence discarded if it cannot produce a better solution than the best one found so far by the algorithm.

In our application, the root of the branch-and-bound tree is associated to the full set of deterministic policies, while a branch is obtained by selecting a couple $(s,a)$ of state and action and subsequently imposing the following disjunction on the two child nodes:
\begin{itemize}
\item $f_{s,a'}=0, \forall a'\neq a$ for the ``left'' childred.
\item $f_{s,a}=0$ for the ``right'' childred. 
\end{itemize} 

The disjunctions imposes to the left child to represent only deterministic policies with $f_{s,a}\neq 0 $  (i.e. $\pi_{s,a}=1$). On the other hand, the right child represents deterministic p with $f_{s,a}=0$  (i.e. $\pi_{s,a}=0$).
The total number of choices (i.e., the number of state-action pairs) is finite, therefore also the maximum size of the branch-and-bond tree is finite. In Figure~\ref{fig:pic_bb} we present an example of a branch-and-bound tree for an MDP with $4$ states and $3$ actions. 
%In this representation, we show at each node the additional restriction to the region 
 
\begin{center}
\input{pic_bb.tex} 
\end{center}

As already mentioned, to avoid exploring the whole tree, we need a lower bounding procedure to \textit{prune} some of the nodes that do not contain the optimal policy. In our application, we use the optimal stochastic policy as underestimator of the optimal deterministic policy for a given branch of the tree. In this way, if a node has a stochastic policy higher than the best deterministic policy found so far it not necesssary to continue explore in that branch and the node ca be pruned.

The final ingredient of a branch-and-bound is a procedure to find feasible deterministic policies. In our implementation, every time that a stochastic policy computed in the bounding procedures is also deterministic we can update the value of the best known deterministic solution to its value.


\input{basic_bb.tex}

In Figure~\ref{fig:basic_bb} we show the pseudo-code of our implementation of the branch-and-bound algorithm. The algorithm starts by initializing the value of the best known deterministic policy to $+\infty$ and the list of unexplred nodes to the root node (i.e., the one with no constraints on the $f$ variables).
The while loop extract one unexplored node from the list, fix the $f$ corresponding to its subregion of feasible deterministic policies and compute a lower bound with Benders decomposition. If the resulting optimal stochastic policy has a maximum regret $\delta^*$ greater or equal than the lower maximum regret found so far for a deterministic policy, no additional nodes are created and the loop extracts another node from the list. if the node is not pruned but the stochastic solution is deterministic, the value of the best deterministic solution is updated to $\delta^*$. As last option, if the stochastic solution is not deterministic, a state $s$ with more thant one $f$ different from zero is found and the $f^*_{s,a}$ with the highest value is selected to be next disjunction used the create the two child nodes.


\input{comparison.tex}

\input{computation.tex}

 
%%================================================================
\section{Conclusions and Perspectives}
		


\section*{Acknowledgement}

\bibliographystyle{splncs04}
\bibliography{biblio}


\end{document}
