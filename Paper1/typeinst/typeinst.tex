
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\usepackage{named}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[numbers]{natbib}
\usepackage{mathtools}

\usepackage{tabularx,ragged2e}
\newcolumntype{C}{>{\Centering\arraybackslash}X} % centered "X" column

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usepackage{tikzsymbols}
\usetikzlibrary{positioning,arrows,arrows.meta}
\usetikzlibrary{shapes.misc}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\up}[1]{\textsuperscript{#1}}
\newcommand{\imp}[1]{{\color{red}{#1}}}
\newcommand{\PA}[1]{{\color{green}{#1}}}
\newcommand{\tfidf}{\emph{tf-idf}}
\newcommand{\NMF}{\emph{NMF}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newlength{\mylengthleft}
\setlength{\mylengthleft}{1ex}
\newlength{\mylength}
\setlength{\mylength}{\textwidth}
\addtolength{\mylength}{-\mylengthleft}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{xcolor}
\usepackage{url}

\urldef{\mailsa}\path|pegah.alizadeh@unicaen.fr|
\urldef{\mailsb}\path|{emiliano.traversi, ao}@lipn.univ-paris13.fr|
\urldef{\mailsc}\path|peggy.cellier@irisa.fr|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}
\newcommand{\ea}[1]{#1 \emph{et al.}}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Deterministic Solutions Based on Maximum Regrets in Markov Decision Processes with Imprecise Rewards}

% a short form should be given in case it is too long for the running head
\titlerunning{Deterministic Solutions for IRMDPs}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%

\author{Pegah Alizadeh \up{1} \and Emiliano Traversi \up{2} \and Aomar Osmani\up{2}}

%
\authorrunning{Pegah Alizadeh, Emiliano Traversi and Aomar Osmani}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published

\institute{(1) GREYC, UMR 6072, UNICAEN/CNRS/ENSICAEN, 14000 CAEN, FRANCE\\
(2) LIPN-UMR CNRS 7030, PRES Sorbonne Paris-cit\'e, FRANCE\\
\mailsa\\
\mailsb \\
%\url{http://www.springer.com/lncs}
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
In several real world situations a stochastic policy is not easily interpretable for system users, this might be due to the nature of the problem or to the system requirements. In these contexts, it is inevitable to provide a deterministic policy to the user. In this paper, we propose an exact method for computing a deterministic policy in reasonable computational overhead in comparison to the required time for computing the optimal stochastic policy. To better motivate the use of an exact procedure for finding a deterministic policy, we show some cases where the intuitive idea of using a deterministic policy obtained after ``rounding" the optimal stochastic policy leads to a deterministic policy different from the optimal.
\keywords{Markov Decision Process, Minimax Regret, Unknown Rewards, Deterministic Policy}
\end{abstract}


\section{Introduction}


%%================================================================
\section{Related Work}

%%================================================================

%This section concerns the required components of MDPs and minimax regret criterion for MDPs with imprecise rewards.

\section{Preliminaries}\label{sec:Preliminaries}
\textit{Markov Decision Process (MDP)} \citep{Puterman1994} is defined by a tuple $M(S, A, P, r, \gamma, \beta)$, where: $S$ is a finite set of states; $A$ is finite set of actions, $P: S \times A \times S \longrightarrow [0,1]$ is a \textit{transition function} where $P(s'|s,a)$ encodes the probability of going to state $s'$ by being in state $s$, and choosing action $a$; $r: S \times A \longrightarrow \mathbb{R}$ is a \textit{reward function} (or penalty, if negative) obtained by choosing action $a$ in state $s$; $\gamma \in [0, 1[$ is the discount factor; and $\beta: S \longrightarrow [0,1]$ is an \textit{initial state distribution function} indicating probability of initiating in state $s$ by $\beta(s)$.

A (stationary) \textit{deterministic policy} is a function $\pi: S \longrightarrow A$, which prescribes to take action $\pi(s)$ when in state $s$. A (stationary) \textit{stochastic policy} is a function $\tilde{\pi}: S \times A \longrightarrow [0,1]$ which indicates with probability $\tilde{\pi} (s,a)$, action $a$ is chosen in state $s$ according to policy $\tilde{\pi}$. A policy $\pi$ induces a \textit{visitation frequency function} $f^{\tilde{\pi}}$ where $f^{\tilde{\pi}}(s,a)$ is the total discounted joint probability of being in state $s$ and choosing action $a$ (see Section $6.9$ in \cite{Puterman1994}):
\begin{align*}
f^{\tilde{\pi}}(s, a) = \sum_{s' \in S} \beta(s') \sum_{t=0}^{\infty} \gamma^{t-1}(S_t = s', A_t = a | S_1 = s)
\end{align*}
where the sum is taken over trajectories defined by $S_0 \sim \beta, A_t \sim \tilde{\pi}(S_t)$ and $S_{t+1} \sim P(.|S_t,A_t)$. The policy is computable from $f^{\tilde{\pi}}$, via 
\begin{align}\label{pi_f}
\tilde{\pi}(s,a) = \frac{f^{\tilde{\pi}}(s, a)}{\sum_{a'} f^{\tilde{\pi}} (s,a')}\;.
\end{align}
For a deterministic policies we have that $f^{\pi}(s,a)= 0$, $\forall a \neq \pi(s)$.\\
Policies are evaluated by expectation of discounted sum of rewards w.r.t to the infinite horizon discounted criterion, namely \textit{value function} $V: S \longrightarrow \mathbb{R}$: 
$V^{\tilde{\pi}}(s) = \mathbb{E}(\sum_{t=0}^{\infty} \gamma^{t}$ $r(s_t, \tilde{\pi}(s_t))$. %Taking into account the initial distribution $\beta$, each policy has an expected value function equal:
%\begin{align*}
%\mathbb{E}_{\sim \beta}[V^{\tilde{\pi}}(s)]=  \sum_{s \in S} \beta(s)V^{\tilde{\pi}}(s) = \beta \cdot V^{\tilde{\pi}}
%\end{align*}
Another way for defining the quality of policies is the \textit{Q-value function}   $Q: S \times A \longrightarrow \mathbb{R}$ given by:
\begin{align}\label{q-v}
Q^{\tilde{\pi}}(s, a) = r(s, a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\tilde{\pi}}(s')\;.
\end{align}

For a give intial state $\beta$, the value of the optimal policy is $\beta \cdot V^{\tilde{\pi}}$, this quantity can be expressed in terms of the visitation frequency function (see \cite{Puterman1994}): 
%The visitation frequency function and value function can be exchanged to each other:
\begin{align}\label{f-v}
%(\sum_{s \in S} \beta(s)V^{\tilde{\pi}}(s)) \;\; \;\;  
\beta \cdot V^{\tilde{\pi}} = r \cdot f^{\tilde{\pi}}\;.
%\;\;\;\;  (\sum_{s \in S} \sum_{a \in A} r(s,a) f^{\tilde{\pi}}(s,a) )
\end{align}
An MDP always has an optimal policy $\pi^*$ such that; $\pi^* = \text{argmax}_{\pi} \beta \cdot V^{\pi}$ or $f^{*} = \text{argmax}_{f} r \cdot f$ where the optimal policy can be recovered from $f^*$ using Equation \ref{pi_f}. 
%%================================================================

\subsection{MDPs with Uncertain Rewards}
When designing real cases as MDPs, specifying the reward function is generally a hard problem. For instance preferences stating which (state, action) pairs are good or bad should be interpreted into numerical costs. Note that even knowing all these preferences is time consuming. In order to
tackle this complexity, we use an MDP with \textit{imprecise reward values (IRMDP)}. An IRMDP \citep{Regan2009} is a tuple $\tilde{M}(S, A, P, \tilde{r}, \gamma, \beta)$ where $S, A, P, \gamma$ and $\beta$ are the same as MDP while $\tilde{r}$ is a set of possible reward functions on $S \times A$. $\tilde{r}$ models the uncertainty on real reward values. To stay coherent, we use the notation presented by \shortcite{benavent2018}, i.e. $M, \tilde{M}, r, \tilde{r}$ signify the standard MDP, IRMDP, real valued reward function and uncertain reward function model respectively.  

Similar to several previous works in the literature \cite{Ahmed2017,alizadeh2015,benavent2018,Regan2009,Weng2013}, we assume that $\tilde{r}$ is given as a polytope. For each $r(s,a) \in \tilde{r}$, the reward uncertainty is restricted in an interval. Thus $\tilde{R}$ is modelled as polytope $C \cdot \overrightarrow{r} \leq \overrightarrow{d}$ where $C$ is $k \times |S||A|$ dimension matrix, $\overrightarrow{d}$ is a $k$ dimensional column vector and $\overrightarrow{r} = (r(s_0,a_0), r(s_0,a_1), \cdots, r(s_0,a_{|A|}), \cdots, r( s_{|S|},a_0), r(s_{|S|},a_1), \cdots, r(s_{|S|},a_{|A|}) )$
 

\subsection{Minimax Regret}
In order to solve the IRMDP $\tilde{M}$ with imprecise rewards bounded in the $\mathcal{R}$ polytope, we use the \textit{minimax regret criterion} method applied in some sequential-decision problems \cite{Regan2009,Xu2009}. Minimax regret is a robust optimization method in presence of uncertain data for approximating optimal policies. 

\begin{definition}
The \textbf{regret} of policy $f^{\pi}$ (has an equivalent policy $\pi$ according to Equation \ref{pi_f}) over reward function $r \in \mathcal{R}$ is defined as the loss or difference in value between f and the optimal policy under $r$: $R(f^{\pi}, r) = \text{max}_{g} \; r \cdot g - r \cdot f$
\end{definition}

\begin{definition}
The \textbf{maximum regret} for policy $f^{\pi}$ is the maximum regret of this policy w.r.t the reward set $\mathcal{R}$: $MR(f^{\pi}, \mathcal{R}) = \text{max}_{r \in \mathcal{R}}\;R(f^{\pi},r)$ 
\end{definition}

In other words, when we should select the $f$ policy, what is the worst case lost over all possible rewards $\mathcal{R}$. Considering it as a game, the adversary tries to find a reward value in order to maximise our lost.  

\begin{definition}
\textbf{minimax regret} of feasible reward set $\mathcal{R}$ is defined as:\\ $MM(\mathcal{R}) = \text{min}_{f^{\pi}}\; MR(f^{\pi}, r)$
\end{definition}

Any policy $f^*$ (its equivalent $\pi^*$) that maximises the minimum regret is the \textit{minmax-regret optimal policy} for the $\tilde{M}$. There are several approaches for computing the minimax regret \cite{alizadeh2015,benavent2018,Regan2009,daSilva2011,Xu2009}. In this paper we use the approach presented by Regan and Boutilier \citep{Regan2009} namely \textit{Benders Decomposition} \cite{Benders1962} to approximate the optimal minimimax regret policy. The idea is to formulate the problem as series of linear programs (LPs) and Mixed Integer Linear Programs (MILPs):

%----------------- minmax regret model --------------------

\begin{center}\label{minimax}
%%%%%
\texttt{Master Program}
%%%%%%
\begin{alignat}{3}
&\text{minimize}_{\delta, f} && \delta & \\
&\text{subject to:}&\quad& r\cdot g - r \cdot f \leq \delta \quad \forall \langle g_r, r \rangle \in \text{GEN}\label{delta_cut}\\
&& \quad& \gamma E^{\top} f + \beta = 0 
\end{alignat}
%%%%%%%
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center} 
%%%%%%%%
\texttt{Slave Program}
\begin{alignat}{3}
&\text{maximize}_{Q, V, I, r} && \beta \cdot V - r \cdot f \\
&\text{subject to:} &\quad& Q_a = r_a + \gamma P_aV &\quad \forall a \in A\\
&& \quad& V \geq Q_a  &\quad \forall a \in A\\
&& \quad& V \leq (1-I_a)M_a + Q_a  &\quad \forall a \in A\\
&& \quad& Cr \leq d \\
&& \quad& \sum_{a \in A} I_a = 1  \label{eq:sum_I}\\
&& \quad& I_a(s) \in \{0, 1 \} &\quad \forall s \in S, \; a \in A \label{eq:bin_I}\\
&& \quad& M_a = M^{\top} - M_a^{\perp} &\quad \forall a \in A
\end{alignat}
%%%%%%
\end{center}
%----------------- minmax regret model --------------------

The master program is a linear program computing the minimum regret with respect to all the possible combinations of rewards and adversary policies. We call GEN the set containg all the combinations of rewards and adversary policies. 
In the first set of constraints, one constraints for each element of GEN $\langle g_r, r \rangle \in \text{GEN}$ is considered. 
%These are pairs $\langle$ policy, reward $\rangle$s wining against policy $f$. 
The second set of constraints of the master problem, $\gamma E ^{\top}f+ \beta = 0$ guaranties that $f$ is a valid visitation frequency function. For the sake of abbreviation, $E$ matrix is generated according to the transition function $P$; $E$ is a $|S||A| \times |S|$-matrix with a row for each state action, and one column for each state:
\[   
E_{sa,s'} = 
     \begin{cases}
       P(s'|s, a) &\quad \text{if } s' \neq s\\
       P(s'|s, a) - \frac{1}{\gamma} &\quad \text{if } s' = s
     \end{cases}
\] 

The intuition behind this constraint is related to the dual linear program of the Bellman Equation (see for example \cite{Sutton1998}, Chapter $4$ or \citep{Puterman1994}, Section $6.9$). 

From a practical point of view, it is uncovenient to enumerate a priori all the constraints~\eqref{delta_cut}. Benders decomposition is base on the idea of starting with a small (maybe empty) subset of constraints~\eqref{delta_cut} and interact with the slave problem to have either a certificate of optimality of the master problem or a new inequality that ca potentially change the value of the master.

The slave program receives a feasible policy $f^*$ and a minimax regret value $\delta^*$; then it searches for a policy and a reward value with the max regret regarding to the given policy $f$, in other words, it finds a $\bar{r}$ and $\bar{g}$ such that $\bar{r} \cdot \bar{g}  - \bar{r}  \cdot f^* > \delta^*$. If such a $(\bar{r},\bar{g})$ is found, it is added to GEN and the master problem is solved again. If this is not the case, the procedure stops and $f^*$ is the (stochastic) policy that minimizes the maximum regret. 

The interaction between master and slave programs can be viewed as a game between two players. The master programs finds an optimal policy minimising regrets w.r.t the given adversaries found so far by the slave program, while the slave program searches for an adversary with the maximum gain against the master policy proposition $f$. This game continues until the slave problem can not find neither a policy as $g$ nor a reward as $r$ to generate a higher regret for the given $f$ by the master program.  

The slave problem is a reformulation of the MR($f, \mathcal{R}$) for the received policy $f$ from the master program. According to Equation \ref{f-v}, $r \cdot g - r \cdot f$ can be rewritten as $\beta \cdot V - r \cdot f$. Thus instead of finding the visitation frequency function as $g$, the value function $V$ related to the adversary policy should be computed. Among the presented constraints for the slave program, constraint $(5)$ ensures that $\forall s \quad Q(s, a) = r(s,a) + \gamma \sum_{s'} P(s'|s,a)V(s')$ (see Equation \ref{q-v}). On the other hand constraints $8$ and $9$ guaranty that $\forall s \quad Q(s, a) = V(s)$ and only one action $a$; $\forall a \quad M_a$ is sufficiently large to tight an upper bound for this equality. $M^{\top}$ is value of the optimal policy for maximum reward values i.e. in our case we compute the optimal policy for $\tilde{M}(S, A, P, r_{\text{max}}, \gamma, \beta)$ where $r_{\text{max}}(s, a) = r_l \quad \text{if} \; r_l \leq r(s, a) \leq r_u$; this can be found using the classical methods such as value iteration or policy iteration \cite{Sutton1998}. Similarly $M^{\perp}$ is the Q-value for the optimal policy with the worst rewards on $\mathcal{R}$. 

Regarding to constraints~\eqref{eq:sum_I} and~\eqref{eq:bin_I} $I$ is a $|S|\times|A|$-matrix defining the policy related to $V$. This policy is a deterministic policy with one and only one selected action $a$ per state $s$. Notice that the slave program proposes a deterministic adversary to the master program while the master program always approximate a stochastic policy. Since the adversary policy proposes an extreme policy w.r.t the given $f$, a MILP model for the slave program is sufficient.   

%%================================================================

\section{An exact Enumerative scheme to find the optimal deterministic solution} 

To find the optimal deteministic solution,  we incorporate the Benders decomposition procedure described in the previous section in a branch-and-bound framework (see for example \cite{bertsimas2005optimization}, Section 11). 

A branch-and-bound algorithm consists of a clever enumeration of the space of feasibile policies through a space search: the set of deterministic policies that can potentially be the optimal is represented with a rooted tree with the full set at the root. The algorithm explores branches of this tree, where each branch represents a subset of the solution set.
Once a new branch of the tree is created, and before that branches is splitted again in additional subbranches, a (lower) bounding procedure is executed on that branch. The bounding procedure gives an underestimation of the optimal solution of the problem over the feasible set associated to the given branch.
The branch is hence discarded if it cannot produce a better solution than the best one found so far by the algorithm.

In our application, the root of the branch-and-bound tree is associated to the full set of deterministic policies, while a branch is obtained by selecting a couple $(s,a)$ of state and action and subsequently imposing the following disjunction on the two child nodes:
\begin{itemize}
\item $f_{s,a'}=0, \forall a'\neq a$ for the ``left'' childred.
\item $f_{s,a}=0$ for the ``right'' childred. 
\end{itemize} 

The disjunctions imposes to the left child to represent only deterministic policies with $f_{s,a}\neq 0 $  (i.e. $\pi_{s,a}=1$). On the other hand, the right child represents deterministic p with $f_{s,a}=0$  (i.e. $\pi_{s,a}=0$).
The total number of choices (i.e., the number of state-action pairs) is finite, therefore also the maximum size of the branch-and-bond tree is finite. In Figure~\ref{fig:pic_bb} we present an example of a branch-and-bound tree for an MDP with $4$ states and $3$ actions. 
%In this representation, we show at each node the additional restriction to the region 
 
\begin{center}
\input{pic_bb.tex} 
\end{center}

As already mentioned, to avoid exploring the whole tree, we need a lower bounding procedure to \textit{prune} some of the nodes that do not contain the optimal policy. In our application, we use the optimal stochastic policy as underestimator of the optimal deterministic policy for a given branch of the tree. In this way, if a node has a stochastic policy higher than the best deterministic policy found so far it not necesssary to continue explore in that branch and the node ca be pruned.

The final ingredient of a branch-and-bound is a procedure to find feasible. deterministic policies. In our implementation, every time that a stochastic policy computed in the bounding procedures is also deterministic we can update the value of the best known deterministic solution to its value.


\input{basic_bb.tex}

In Figure~\ref{fig:basic_bb} we show the pseudo-code of our implementation of the branch-and-bound algorithm. The algorithm starts by initializing the value of the best known deterministic policy to $+\infty$ and the list of unexplred nodes to the root node (i.e., the one with no constraints on the $f$ variables).
The while loop extract one unexplored node from the list, fix the $f$ corresponding to its subregion of feasible deterministic policies and compute a lower bound with Benders decomposition. If the resulting optimal stochastic policy has a maximum regret $\delta^*$ greater or equal than the lower maximum regret found so far for a deterministic policy, no additional nodes are created and the loop extracts another node from the list. if the node is not pruned but the stochastic solution is deterministic, the value of the best deterministic solution is updated to $\delta^*$. As last option, if the stochastic solution is not deterministic, a state $s$ with more thant one $f$ different from zero is found and the $f^*_{s,a}$ with the highest value is selected to be next disjunction used the create the two child nodes.

arily bad. 	


\section{Theoretical analysis of the optimal deterministic}

In this section we first introduce the intuitve concept of rounding heuristic, a way to obtain a feasible determinsitci policy starting from a stochastic (maybe optimal) policy. Subsequently we analize the situations where such rounding heuristic could provide maximum regrets far from the one gived by the optimal deterministic policy.

\subsection{The ``rounding'' heuristic}

Let $\bar{f}$ be a given stochastic optimal policy. The corresponding ``rounded'' deterministic policy can be computed as follows:
\begin{itemize}
\item for each $s'\in S$:
\begin{itemize}
\item find the action $a' = argmax_{a \in A}f_{s',a}$.
\item fix the rest of the action to zero: $f_{s',a} =0, \forall a \neq a'$
\end{itemize}
\item compute the value of the deterministic policy obtained with the given fixing.
\end{itemize}
 
The heuristic searches --for each state-- the action with the highest probability to be chosen. It selects that action as part of the deterministic policy.
Despite being pretty simple, the proposed heuristic represents a plausible behaviour of a user that want to deduce a deterministic policy starting from a stochastic one.  
%
%For the \textit{Tiny MDP} presented in Bruno's manuscript, it is easy to check that the rounding heuristic always gives the optimal deterministic policy (i.e., the one that minimizes the max regrets).
%
%What we would like to find is a class of instances where this is not the case or, even better, instances where the difference between the heuristic deterministic policy and the optimal deterministic policy can be arbitr
%
%For a given node of the branch and 
%As bounding procedure   
%
%All the tests so far showed that the vast majority of the benders inequalities are added in the computation of the root node of the branch-and-bound (i.e., in the computation of the stochastic policy), we hope that in this way the time spent in the enumeration of the branch-and-bound tree will be reasonable.
%
\subsection{A small counterexample}

\subsubsection{Tiny MDP}
We call \textit{Tiny MDP} the MDP defined to have:
\begin{itemize}
\item Three states: $q, s_0, s_1$; and three actions $a_0, a_1, a_2$.
\item Two deterministic transiction functions (associated to $a_0$ and $a_1$): $T(q,a_0,s_0)=1$ and $T(q,a_1,s_1)=1$; and one stochastic transiction function with $T(q,a_2,s_0)=T_0$ and $T(q,a_2,s_1)=T_1$. 	 
\item Two unknow rewards associated to $s_0$ and $s_1$: $r_{a_0}= r_0\in[-A,+A]$ and $r_{a_1}= r_1\in[-A+B,+A+B]$ with $A,B \in \mathbb{R}_+$ and $A \gg B$.
\end{itemize} 

\begin{proposition}
If $T_1\geq T_0$, The optimal deterministic policy is $\pi_{a_2}=1$, $\pi_{a_0}=\pi_{a_1}=0$. 
\end{proposition}
\begin{proof}
We proof the statement by explicitly computing the maximum regret of the three possible deterministico policy: $\pi_{a_0}=1$, $\pi_{a_1}=1$ and $\pi_{a_2}=1$.\\
Maximum regret of $\pi_{a_0}=1$: to compute the maximum regret associated to $\pi_{a_0}=1$, we considere the   
Maximum regret of $\pi_{a_1}=1$.
Maximum regret of $\pi_{a_2}=1$.
\end{proof}



\section{}
 
%%================================================================
\section{Conclusions and Perspectives}
		


\section*{Acknowledgement}

\bibliographystyle{splncs04}
\bibliography{biblio}


\end{document}
