
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\usepackage{named}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[numbers]{natbib}
\usepackage{mathtools}

\usepackage{tabularx,ragged2e}
\newcolumntype{C}{>{\Centering\arraybackslash}X} % centered "X" column


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\up}[1]{\textsuperscript{#1}}
\newcommand{\imp}[1]{{\color{red}{#1}}}
\newcommand{\PA}[1]{{\color{green}{#1}}}
\newcommand{\tfidf}{\emph{tf-idf}}
\newcommand{\NMF}{\emph{NMF}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{xcolor}
\usepackage{url}

\urldef{\mailsa}\path|pegah.alizadeh@unicaen.fr|
\urldef{\mailsb}\path|{emiliano.traversi, ao}@lipn.univ-paris13.fr|
\urldef{\mailsc}\path|peggy.cellier@irisa.fr|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}
\newcommand{\ea}[1]{#1 \emph{et al.}}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Deterministic Solutions Based on Maximum Regrets in Markov Decision Processes with Imprecise Rewards}

% a short form should be given in case it is too long for the running head
\titlerunning{Deterministic Solutions for IRMDPs}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%

\author{Pegah Alizadeh \up{1} \and Emiliano Traversi \up{2} \and Aomar Osmani\up{2}}

%
\authorrunning{Pegah Alizadeh, Emiliano Traversi and Aomar Osmani}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published

\institute{(1) GREYC, UMR 6072, UNICAEN/CNRS/ENSICAEN, 14000 CAEN, FRANCE\\
(2) LIPN-UMR CNRS 7030, PRES Sorbonne Paris-cit\'e, FRANCE\\
\mailsa\\
\mailsb \\
%\url{http://www.springer.com/lncs}
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
In several real world situations a stochastic policy is not easily interpretable for system users, this might be due to the nature of the problem or to the system requirements. In these contexts, it is inevitable to provide a deterministic policy to the user. In this paper, we propose an exact method for computing a deterministic policy in reasonable computational overhead in comparison to the required time for computing the optimal stochastic policy. To better motivate the use of an exact procedure for finding a deterministic policy, we show some cases where the intuitive idea of using a deterministic policy obtained after ``rounding" the optimal stochastic policy leads to a deterministic policy far from the optimal.
\keywords{Markov Decision Process, Minimax Regret, Unknown Rewards, Deterministic Policy}
\end{abstract}


\section{Introduction}


%%================================================================
\section{Related Work}

%%================================================================

%This section concerns the required components of MDPs and minimax regret criterion for MDPs with imprecise rewards.

\section{Preliminaries}\label{sec:Preliminaries}
\textit{Markov Decision Process (MDP)} \citep{Puterman1994} is defined by a tuple $M(S, A, P, r, \gamma, \beta)$, where: $S$ is a finite set of states; $A$ is finite set of actions, $P: S \times A \times S \longrightarrow [0,1]$ is a \textit{transition function} where $P(s'|s,a)$ encodes the probability of going to state $s'$ by being in state $s$, and choosing action $a$; $r: S \times A \longrightarrow \mathbb{R}$ is a \textit{reward function} (or penalty, if negative) obtained by choosing action $a$ in state $s$; $\gamma \in [0, 1[$ is the discount factor; and $\beta: S \longrightarrow [0,1]$ is an \textit{initial state distribution function} indicating probability of initiating in state $s$ by $\beta(s)$.

A (stationary) \textit{deterministic policy} is a function $\pi: S \longrightarrow A$, which prescribes to take action $\pi(s)$ when in state $s$. A (stationary) \textit{stochastic policy} is a function $\tilde{\pi}: S \times A \longrightarrow [0,1]$ which indicates with probability $\tilde{\pi} (s,a)$, action $a$ is chosen in state $s$ according to policy $\tilde{\pi}$. A policy $\pi$ induces a \textit{visitation frequency function} $f^{\tilde{\pi}}$ where $f^{\tilde{\pi}}(s,a)$ is the total discounted joint probability of being in state $s$ and choosing action $a$ \cite{Puterman1994} (section$6.9$):
\begin{align*}
f^{\tilde{\pi}}(s, a) = \sum_{s' \in S} \beta(s') \sum_{t=0}^{\infty} \gamma^{t-1}(S_t = s', A_t = a | S_1 = s)
\end{align*}
where the sum is taken over trajectories defined by $S_0 \sim \beta, A_t \sim \tilde{\pi}(S_t)$ and $S_{t+1} \sim P(.|S_t,A_t)$. The policy is computable from $f^{\tilde{\pi}}$, via 
\begin{align}\label{pi_f}
\tilde{\pi}(s,a) = \frac{f^{\tilde{\pi}}(s, a)}{\sum_{a'} f^{\tilde{\pi}} (s,a')}
\end{align}
For deterministic policies $\forall a \neq \pi(s) \; f^{\pi}(s,a)= 0$.\\
Policies are evaluated by expectation of discounted sum of rewards w.r.t to the infinite horizon discounted criterion namely \textit{value function} $V: S \longrightarrow \mathbb{R}$: 
$V^{\tilde{\pi}}(s) = \mathbb{E}(\sum_{t=0}^{\infty} \gamma^{t}$ $r(s_t, \tilde{\pi}(s_t))$. %Taking into account the initial distribution $\beta$, each policy has an expected value function equal:
%\begin{align*}
%\mathbb{E}_{\sim \beta}[V^{\tilde{\pi}}(s)]=  \sum_{s \in S} \beta(s)V^{\tilde{\pi}}(s) = \beta \cdot V^{\tilde{\pi}}
%\end{align*}
Another way for defining quality of policies is the \textit{Q-value function}   $Q: S \times A \longrightarrow \mathbb{R}$ given by:
\begin{align}\label{q-v}
Q^{\tilde{\pi}}(s, a) = r(s, a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\tilde{\pi}}(s')
\end{align}

The visitation frequency function and value function can be exchanged to each other:
\begin{align}\label{f-v}
%(\sum_{s \in S} \beta(s)V^{\tilde{\pi}}(s)) \;\; \;\;  
\beta \cdot V^{\tilde{\pi}} = r \cdot f^{\tilde{\pi}}
%\;\;\;\;  (\sum_{s \in S} \sum_{a \in A} r(s,a) f^{\tilde{\pi}}(s,a) )
\end{align}
An MDP always has an optimal policy $\pi^*$ such that; $\pi^* = \text{argmax}_{\pi} \beta \cdot V^{\pi}$ or $f^{*} = \text{argmax}_{f} r \cdot f$ where the optimal policy can be recovered from $f^*$ using Equation \ref{pi_f}. 
%%================================================================

\subsection{MDPs with Uncertain Rewards}
When designing real cases as MDPs, specifying the reward function is generally a hard problem. For instance preferences stating which (state, action) pairs are good or bad should be interpreted into numerical costs. Note that even knowing all these preferences is time consuming. In order to
tackle this complexity, we use an MDP with \textit{imprecise reward values (IRMDP)}. An IRMDP \citep{Regan2009} is a tuple $\tilde{M}(S, A, P, \tilde{r}, \gamma, \beta)$ where $S, A, P, \gamma$ and $\beta$ are the same as MDP while $\tilde{r}$ is a set of possible reward functions on $S \times A$. $\tilde{r}$ models the uncertainty on real reward values. To stay coherent, we use the notation presented by \shortcite{benavent2018}, i.e. $M, \tilde{M}, r, \tilde{r}$ signify the standard MDP, IRMDP, real valued reward function and uncertain reward function model respectively.  

Similar to several previous works in the literature \cite{Ahmed2017,alizadeh2015,benavent2018,Regan2009,Weng2013}, we assume that $\tilde{r}$ is given as a polytope. For each $r(s,a) \in \tilde{r}$, the reward uncertainty is restricted in an interval. Thus $\tilde{R}$ is modelled as polytope $C \cdot \overrightarrow{r} \leq \overrightarrow{d}$ where $C$ is $k \times |S||A|$ dimension matrix, $\overrightarrow{d}$ is a $k$ dimensional column vector and $\overrightarrow{r} = (r(s_0,a_0), r(s_0,a_1), \cdots, r(s_0,a_{|A|}), \cdots, r( s_{|S|},a_0), r(s_{|S|},a_1), \cdots, r(s_{|S|},a_{|A|}) )$
 

\subsection{Minimax Regret}
In order to solve the IRMDP $\tilde{M}$ with imprecise rewards bounded in the $\mathcal{R}$ polytope, we use the \textit{minimax regret criterion} method applied in some sequential-decision problems \cite{Regan2009,Xu2009}. Minimax regret is a robust optimization method in presence of uncertain data for approximating optimal policies. 

\begin{definition}
The \textbf{regret} of policy $f^{\pi}$ (has an equivalent policy $\pi$ according to Equation \ref{pi_f}) over reward function $r \in \mathcal{R}$ is defined as the loss or difference in value between f and the optimal policy under $r$: $R(f^{\pi}, r) = \text{max}_{g} \; r \cdot g - r \cdot f$
\end{definition}

\begin{definition}
The \textbf{maximum regret} for policy $f^{\pi}$ is the maximum regret of this policy w.r.t the reward set $\mathcal{R}$: $MR(f^{\pi}, \mathcal{R}) = \text{max}_{r \in \mathcal{R}}\;R(f^{\pi},r)$ 
\end{definition}

In other words, when we should select the $f$ policy, what is the worst case lost over all possible rewards $\mathcal{R}$. Considering it as a game, the adversary tries to find a reward value in order to maximise our lost.  

\begin{definition}
\textbf{minimax regret} of feasible reward set $\mathcal{R}$ is defined as:\\ $MM(\mathcal{R}) = \text{min}_{f^{\pi}}\; MR(f^{\pi}, r)$
\end{definition}

Any policy $f^*$ (its equivalent $\pi^*$) that maximises the minimum regret is the \textit{minmax-regret optimal policy} for the $\tilde{M}$. There are several approaches for computing the minimax regret \cite{alizadeh2015,benavent2018,Regan2009,daSilva2011,Xu2009}. In this paper we use the approach presented by Regan and Boutilier \citep{Regan2009} namely \textit{Benders Decomposition} \cite{Benders1962} to approximate the optimal minimimax regret policy. The idea is to formulate the problem as series of linear programmings (LPs) and Mixed Integer Linear Programmings (MILPs):

%----------------- minmax regret model --------------------

\begin{center}\label{minimax}
%%%%%
\texttt{Master Program}
%%%%%%
\begin{alignat}{3}
&\text{minimize}_{\delta, f} && \delta & \\
&\text{subject to:} &\quad& r\cdot g - r \cdot f \leq \delta \quad \forall \langle g_r, r \rangle \in \text{GEN}\\
&& \quad& \gamma E^{\top} f + \beta = 0 
\end{alignat}
%%%%%%%
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center} 
%%%%%%%%
\texttt{Slave Program}
\begin{alignat}{3}
&\text{maximize}_{Q, V, I, r} && \beta \cdot V - r \cdot f \\
&\text{subject to:} &\quad& Q_a = r_a + \gamma P_aV &\quad \forall a \in A\\
&& \quad& V \geq Q_a  &\quad \forall a \in A\\
&& \quad& V \leq (1-I_a)M_a + Q_a  &\quad \forall a \in A\\
&& \quad& Cr \leq d \\
&& \quad& \sum_{a \in A} I_a = 1  \\
&& \quad& I_a(s) \in \{0, 1 \} &\quad \forall s \in S, \; a \in A\\
&& \quad& M_a = M^{\top} - M_a^{\perp} &\quad \forall a \in A \\
\end{alignat}
%%%%%%
\end{center}
%----------------- minmax regret model --------------------

The master program is a linear program computing the minimum regret w.r.t the given set of adversaries namely \textit{generated constraints} $\langle g_r, r \rangle \in \text{GEN}$ \citep{Regan2009}. These are pairs $\langle$ policy, reward $\rangle$s wining against policy $f$. The second constraint of the master problem, $\gamma E ^{\top}f+ \beta = 0$ guaranties that $f$ is a valid visitation frequency function. For the sake of abbreviation, $E$ matrix is generated according to the transition function $P$; $E$ is a $|S||A| \times |S|$-matrix with a row for each state action, and one column for each state:
\[   
E_{sa,s'} = 
     \begin{cases}
       P(s'|s, a) &\quad \text{if } s' \neq s\\
       P(s'|s, a) - \frac{1}{\gamma} &\quad \text{if } s' = s
     \end{cases}
\] 

The intuition behind this constraint is related to the dual linear program of the Bellman Equation \cite{Sutton1998} chapter $4$ (For more details see \citep{Puterman1994} section $6.9$). 

The slave program receives an optimal policy and a minimax regret value; then it computes a policy and reward value with the max regret regarding to the given policy $f$. The formula can be considered as a game between two players: master and slave programs. The master programs finds an optimal policy minimising regrets w.r.t the given adversaries by the slave program, while the slave program searches for an adversary with the maximum gain against the master policy proposition $f$. This game continues until the slave problem can not find neither a policy as $g$ nor a reward as $r$ to generate a higher regret for the given $f$ by the master program.  

The slave problem is a reformulation of the MR($f, \mathcal{R}$) for the received policy $f$ from the master program. According to Equation \ref{f-v}, $r \cdot g - r \cdot f$ can be rewritten as $\beta \cdot V - r \cdot f$. Thus instead of finding the visitation frequency function as $g$, the value function $V$ related to the adversary policy should be computed. Among the presented constraints for the slave program, constraint $(5)$ ensures that $\forall s \quad Q(s, a) = r(s,a) + \gamma \sum_{s'} P(s'|s,a)V(s')$ (see Equation \ref{q-v}). On the other hand constraints $8$ and $9$ guaranty that $\forall s \quad Q(s, a) = V(s)$ and only one action $a$; $\forall a \quad M_a$ is sufficiently large to tight an upper bound for this equality. $M^{\top}$ is value of the optimal policy for maximum reward values i.e. in our case we compute the optimal policy for $\tilde{M}(S, A, P, r_{\text{max}}, \gamma, \beta)$ where $r_{\text{max}}(s, a) = r_l \quad \text{if} \; r_l \leq r(s, a) \leq r_u$; this can be found using the classical methods such as value iteration or policy iteration \cite{Sutton1998}. Similarly $M^{\perp}$ is the Q-value for the optimal policy with the worst rewards on $\mathcal{R}$. 

Regarding to constraints $(12)$ and $(13)$ $I$ is a $|S|\times|A|$-matrix defining the policy related to $V$. This policy is a deterministic policy with one and only one selected action $a$ per state $s$. Notice that the slave program proposes a deterministic adversary to the master program while the master program always approximate a stochastic policy. Since the adversary policy proposes an extreme policy w.r.t the given $f$, a MILP model for the slave program is sufficient.   

%%================================================================
\section{An exact Enumerative scheme} 
 
%%================================================================
\section{Conclusions and Perspectives}
		

\section*{Acknowledgement}

\bibliographystyle{splncs04}
\bibliography{biblio}


\end{document}
