%%================================================================
\section{Related Work}

Typically in MDPs the reward functions are estimated either from observations or external sources. \shortcite{Mannor2007} demonstrate that the policy found via an optimisation process under the MDPs with numerical parameters, sometimes can be much worse than the anticipated policy. This motivates using MDPs that account for this ambiguity in model parameters. To cope with this problem, first the MDP with imprecise rewards (IRMDP) should be modelled mathematically. There exists several models in the literature including symbolic-based rewards approaches \cite{Furnkranz2012,Weng2012} and numerical-based rewards approaches \cite{bell1982,Regan2009,Xu2009}. We focus on group of approaches taking decision-theoretic point of view, and consider a set of MDPs with different reward functions --modelling under uncertainty-- namely \textit{Imprecise Reward MPDs (IRMDPs)}. 


In order to find the optimal policy under uncertainty, robust solutions typically provides guaranties on the worst case performance. One common approach in computing robust solution is \textit{maximin} method that computes a policy maximising the value with respect to the worst case scenario \cite{Nilim2005,Iyengar2005,GIVAN2000,mastin2012}. Minimax robustness can be considered as a game between two adversaries, one finds a policy with maximum values while the adversary chooses an instantiation of reward functions that will minimise the expected value. There are some recent works that propose some techniques for dependent uncertainties in MDPs \cite{Wiesemann2013,Mannor2012}. This paper setting is that the uncertain reward functions are independent from each other. 

\textit{maximin} policies are conservative naturally \cite{Delage2007}, thus \textit{minimax regret} approach \cite{Regan2009,Xu2009} have been introduced as an approach to cope with this issue. In \textit{minimax} regret criterion the goal is to find a policy that has the least value of maximum regret over all instantiations of rewards. Previous algorithms \cite{Regan2010,Xu2009,Regan2009} have only focused on computing  \textit{optimal stochastic policies} to IRMDPs, i.e. a policy proposing several actions associated with probabilities for a state in IRMDP. To the best of our knowledge there is no work that handle deterministic policy computation on MDPs under uncertainties. The existent works on deterministic policies computation deal usually with MDPs with precise parameters \cite{Dolgov2005}. In this paper we introduce an approach for computing the closest deterministic policy with the minimum value on maximum regret over all instantiations of rewards. 