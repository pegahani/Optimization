xÅ“
\section{Theoretical analysis of the optimal deterministic policy}

In this section we first introduce the intuitve concept of rounding heuristic, a way to obtain a feasible determinsitci policy starting from a stochastic (maybe optimal) policy. Subsequently we analize the situations where such rounding heuristic could provide maximum regrets far from the one gived by the optimal deterministic policy.

\subsection{The ``rounding'' heuristic}

Let $\bar{f}$ be a given stochastic optimal policy. The corresponding ``rounded'' deterministic policy can be computed as follows:
\begin{itemize}
\item for each $s'\in S$:
\begin{itemize}
\item find the action $a' = argmax_{a \in A}f_{s',a}$.
\item fix the rest of the action to zero: $f_{s',a} =0, \forall a \neq a'$
\end{itemize}
\item compute the value of the deterministic policy obtained with the given fixing.
\end{itemize}
 
The heuristic searches --for each state-- the action with the highest probability to be chosen. It selects that action as part of the deterministic policy.
Despite being pretty simple, the proposed heuristic represents a plausible behaviour of a user that want to deduce a deterministic policy starting from a stochastic one.  
%
%For the \textit{Tiny MDP} presented in Bruno's manuscript, it is easy to check that the rounding heuristic always gives the optimal deterministic policy (i.e., the one that minimizes the max regrets).
%
%What we would like to find is a class of instances where this is not the case or, even better, instances where the difference between the heuristic deterministic policy and the optimal deterministic policy can be arbitr
%
%For a given node of the branch and 
%As bounding procedure   
%
%All the tests so far showed that the vast majority of the benders inequalities are added in the computation of the root node of the branch-and-bound (i.e., in the computation of the stochastic policy), we hope that in this way the time spent in the enumeration of the branch-and-bound tree will be reasonable.
%
\subsection{A small counterexample}

We call \textit{Trident MDP} the MDP defined to have:
\begin{itemize}
\item Three states: $q, s_0, s_1$, three actions $a_0, a_1, a_2$ and a discout factor $\gamma=1$.
\item Two deterministic transiction functions (associated to $a_0$ and $a_1$): $T(q,a_0,s_0)=1$ and $T(q,a_1,s_1)=1$; and one stochastic transiction function with $T(q,a_2,s_0)=T_0$ and $T(q,a_2,s_1)=T_1$. 	 
\item Two unknow rewards associated to $s_0$ and $s_1$: $r_{a_0}= r_0\in[-A,+A]$ and $r_{a_1}= r_1\in[-A+B,+A+B]$ with $A,B > 0$ and $A \gg B$.
\item an initial distribution $\beta_{s_0}= \beta_{s_1}=0$ and $\beta_{s_2}=1$.
\end{itemize} 



The following propositions give a complete characterization of the optimal stochastic and deterministic policies for the Trident MDP. With a slight abuse of notation, we use the pedix $a$ instead of using $s_2,s_a$, this is due to the fact that the only relevant actions are the ones originating from the state $s_2$ (for example, we use $\pi_1$ instead of $\pi_{s_2, a_1}$). For the same reason, we use $r_{0}$ in place of $r_{s_0, a_0}$ and $r_{1}$ in place of $r_{s_1, a_0}$.

\begin{proposition}\label{theorem:opt_stoc}
An optimal stochastic policy  for the Trident MDP is the following:
$$\pi_{0}=\dfrac{2A - B}{4A},~~~\pi_{1}=\dfrac{2A + B}{4A}, ~~~\pi_2 = 0$$
(regardless of the values of $T_1$ and $T_2$).
\end{proposition}
\begin{proof}
We first show that alway have an optimal policy with $\pi_2 = 0$. We do this by observing that for every policy $(\pi'_0, \pi'_1, \pi'_2)$ with $\pi'_2 > 0$, it is possible to obtain a policy policy $(\pi''_0, \pi''_1, \pi''_2)$ with $\pi''_2 = 0$ in the following way:

$$\pi''_0 = \pi'_0 + \pi'_2 T_0, ~~~ \pi''_1 = \pi'_1 + \pi'_2 T_1\;.$$

If we compute the value of the first policy we notice that:
\begin{align*}
\beta \cdot V^{\pi'} = V^{\pi'}(s_2) =
r_{_0} \pi'_0 + r_{_1}\pi'_1 + r_{_0} T_0 \pi'_2 + r_{_1} T_1 \pi'_2 \\
= r_{_0} (\pi'_0 + T_0 \pi'_2) + r_{_1} (\pi'_1 + T_1 \pi'_2)
= r_{_0} \pi''_0 + r_{_1}\pi''_1 = V^{\pi''}(s_2) =\beta \cdot V^{\pi''}\;.
\end{align*}
Showing that both policy has same value. We can hence supposte that $\pi_2 =0$ in an optimal stochastic solution.

As second part of the proof, we compute the value of an optimal policy with $\pi_0, \pi_1 \geq 0$.
Let $\pi_0,\pi_1$ be a generic policy. We notice that the adversary policy $g$ giving a maximum regret  is always deterministic (see~\ref{benavent2018}). For this reason, the maximum regret is the maximum among two choices for the adversary policy: we can have either $g_0=g_2=0$ and $g_1 > 0$ or the opposite, $g_0>0$ and $g_1=g_2 = 0$. (We notice that, with arguments analogous to the ones used in the first part of the proof, we can always rule out the case where $g_2 \geq 0$.)

The maximum regret associated to the policy $g_0=g_2=0$ and $g_1 > 0$  (obtained by fixing $r_0 = -A$ and $r_1 = A+B$) is the following
\begin{align}
r \cdot g - r \cdot f = A + B +A \pi_0 -(A+B)\pi_1 \label{eq:best_stoc_1}
\end{align}   
and the maximum regret associated to the policy $g_1=g_2=0$ and $g_0 > 0$ is obtained by fixing $r_0 = A$ and $r_1 = -A+B$, leading to a value of
\begin{align}
r \cdot g - r \cdot f = A - A \pi_0 -(B-A)\pi_1 \label{eq:best_stoc_2}\;.
\end{align}   
We are interested in minmizing the max regret, this measn that we want to find the values of $\pi_0$ and $\pi_1$ that minimize $\max \{\eqref{eq:best_stoc_1}, \eqref{eq:best_stoc_2}\}$. The optimal stochastic policy can hence be obtained by solving the following system of two equations 
\begin{align*}
A + B +A \pi_0 -(A+B)\pi_1 &= A - A \pi_0 -(B-A)\pi_1\\
\pi_0+\pi_1 &= 1
\end{align*} 
That has as optimal solution the values $\pi_{0}=\dfrac{2A - B}{4A}$ and $\pi_{1}=\dfrac{2A + B}{4A}$, concluding the proof.$\square$
\end{proof}


Proposition~\ref{theorem:opt_stoc} implies the following Lemma:
\begin{lemma}\label{lemma:heur_policy}
The rounded deterministic policy for the Trident MDP is $\pi_0 =\pi_2 = 0$ and $\pi_1 = 1$ and its maximum regret is equal to $2A-B$.
\end{lemma}
\begin{proof}
It is a direct consequence of the fact that in the optimal stochastic solution we always have $\pi_1 > \pi_2$ and $\pi_0 = 0$.$\square$
\end{proof}



\begin{proposition}\label{theorem:opt_det}
If $T_1 > T_0$, The optimal deterministic policy is $\pi_{2}=1$, $\pi_{0}=\pi_{1}=0$ 
and its maximum regret is $A- A T_0 +(A-B) T_1$. 
\end{proposition}
\begin{proof}
We proof the statement by explicitly computing the maximum regret of the three possible deterministic policy: $\pi_0=1$, $\pi_1=1$ and $\pi_2=1$.



\textit{Maximum regret of $\pi_0=1$}.\\
We want to find the adversary policy that maximizes the regret for the policy $\pi_0=1$. We do that by computing al possible combinations of adversay policy and reward:
\begin{itemize}
\item If the adversary policy is $g_1> 0$, the reward maximizing the regret is $r_0 = -A$ and $r_1 = A+B$, leading to a maximum regret of 
\begin{align}
A+B-(-A)=2A+B \label{eq:regret0}
\end{align}
\item If the adversary policy is $g_2>0$, we need to check all four combination of extreme rewards:
\begin{itemize}
\item $r_0 = -A$ and $r_1= A+B$. Maximum regret of
\begin{align}
-A T_0 + (A+B)T_1 + A = (1-T_0 + T_1)A + T_1 B \label{eq:regret1}
\end{align}
\item $r_0 = A$ and $r_1= A+B$. Maximum regret of
\begin{align}
A T_0 + (A+B)T_1 - A = (-1+T_0 + T_1)A + T_1 B  \label{eq:regret2}
\end{align}
\item $r_0 = A$ and $r_1= -A+B$. Maximum regret of
\begin{align} 
A T_0 + (-A+B)T_1 - A =  (-1+T_0 - T_1)A + T_1 B \label{eq:regret3}
\end{align}
\item $r_0 = -A$ and $r_1= -A+B$. Maximum regret of
\begin{align}
-A T_0 + (-A+B)T_1 + A =  (1 - T_0 - T_1)A - T_1 B \label{eq:regret4}
\end{align}
\end{itemize} 
We observe that $A\gg B$ and $T_0+T_1= 1$, this imply that  $\eqref{eq:regret0} \geq \max\{\eqref{eq:regret1}-\eqref{eq:regret4} \}$. Therefore, the maximum regret if $g_2>0$ is $2A+B$.
\end{itemize} 




\textit{Maximum regret of $\pi_1=1$}.\\
It is trivial to chech, with calculations analogous to the one used to compute the regret of $\pi_0=1$, that the maximum regret in this case is equal to $2A-B$.\\




\textit{Maximum regret of $\pi_2=1$}.\\
Also in this case, we need to consider the two cases of $g_0 > 0$ and $g_1 > 0$. If $g_0 > 0$ we fix $r_0 = A$ and $r_1 = -A+B$, obtaining a regret equal to 
\begin{align}
A- A T_0 +(A-B) T_1 \label{eq:regret2_1}
\end{align}
And if $g_1 > 0$ we fix $r_0 = -A$ and $r_1 = A+B$, obtaining a regret equal to 
\begin{align}
A+B+A T_0 -(A+B) T_1\;. \label{eq:regret2_2}
\end{align}
The maximum between~\eqref{eq:regret2_1} and~\eqref{eq:regret2_2} depends on the values of $T_0$ and $T_1$.
By imposing  $A- A T_0 +(A-B) T_1 \geq A+B+A T_0 -(A+B) T_1$ we obtain:
$$ 2A T_1 \geq 2 A T_0 + B\;. $$
We recall that by construction we have $A \gg B$, this implies that if $T_1> T_0$ (resp. $T_1\leq T_0$)  we have that the maximum regret is equal to~\eqref{eq:regret2_1} (resp.~\eqref{eq:regret2_2}).
The minimum maximum regret found so far is the one obtained for $\pi_1=1$, and it is equal to $2A-B$. It remains to check for which values of $T_0 > T_1$ we have that $2A-B \geq~\eqref{eq:regret2_1}$:
\begin{align*}
A- A T_0 +(A-B) T_1 \leq 2A - B \Leftrightarrow_{T_0 = 1-T_1} A- A (1-T_1) +(A-B) T_1 \leq 2A - B\\
  \Leftrightarrow (2A-B)T_1 \leq 2A-B  \Leftrightarrow T1 \leq 1\;.
\end{align*}
 Since we have by construction that $T_1 \leq1$ we can conclude that for any $T_1> T_0$ the optimal deterministic policy is obtained when $\pi_{2}=1$ and $\pi_{0}=\pi_{1}=0$ and its maximum regret is equal to $A- A T_0 +(A-B) T_1$. $\square$
\end{proof}


Proposition~\ref{theorem:opt_det} and Lemma~\ref{lemma:heur_policy} shows that for any Trident MDP we have that the optimal deterministic policy and the rounded deterministic policy are always different. In the following Lemma we estimate the value of each policy:


\begin{lemma}
the ratio between the maximum regret of the rounding deterministic policy and the optimal deterministic policy goes to zero with the increase of the value of $A$ with respect to $B$ and the increase of $T_1$. In other words:  
\begin{align*}
\lim_{A/B \rightarrow \infty, T_1 \rightarrow 1} \dfrac{2A-B}{A- A T_0 +(A-B) T_1} = 2
\end{align*}
\end{lemma}