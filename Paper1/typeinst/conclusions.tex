We presented for the first time in the literature an algorithm to find an optimal deterministic policy that minimizes the minimax regret of a Markov Decision Processes (MDP) with imprecise rewards.
The proposed algorithm consists of a branch-and-bound that used Benders decomposition as bounding procedure. In addition to a basic implementation we propose a cut-and-branch implementation that turns out to reduce the overall commputing time on average by $50\%$.  
We motivate the use of deterministic over stochastic policies by showing theoretically that basic rounding procedures find deterministic policies far from the optimal. Secondly, we show that the additional computational effort of computing the optimal deterministic policy in comparison to the one needed to compute the optimal stochastic policy is acceptable (approximately one order of magnitude slower).
We hope that this manuscript motivates the scientific community to investigate more the development of algorithm for deterministic solutions in the context of MDPs with imprecise rewerds. 
