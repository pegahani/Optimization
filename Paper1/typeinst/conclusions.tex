We presented for the first time in the literature a techniques to find an optimal deterministic policy that minimizes the minimax regret of a Markov Decision Processes (MDP) with imprecise rewards.
We motivate the use of deterministic over stochastic policies by first show theoretically that basic rounding procedures find deterministic policies far from the optimal. Secondly, we show that the additional computational effort of computing the optimal deterministic policy in comparison to the one needed to compute the optimal stochastic policy is acceptable (approximately one order of magnitude slower). Finally, we show that with a cut-and-branch implementation of our algorithm we are 2 times faster in comparison with the basic implementation of the algorithm. 