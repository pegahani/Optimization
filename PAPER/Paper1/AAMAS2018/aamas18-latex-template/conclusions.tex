We presented for the first time in the literature an algorithm to find an optimal deterministic policy that minimises the maximum regret of a Markov Decision Processes (MDP) with imprecise rewards.
The proposed algorithm consists of a branch-and-bound that uses Benders decomposition as bounding procedure. In addition to a basic implementation, we propose a cut-and-branch implementation that turns out to reduce the overall computing time on average by $50\%$.  
We motivate the use of deterministic over stochastic policies by showing theoretically that basic rounding procedures find deterministic policies far from the optimal. Secondly, we show that the additional computational effort of computing the optimal deterministic policy in comparison to the one needed to compute the optimal stochastic policy is acceptable (approximately one order of magnitude slower). 

\ET{A possible future research direction could be studying more heuristic algorithms that would allow to handle MDPs of bigger size with less computational time.} We hope that this manuscript motivates the scientific community to investigate more the development of algorithm for deterministic solutions in the context of imprecise MDPs.
