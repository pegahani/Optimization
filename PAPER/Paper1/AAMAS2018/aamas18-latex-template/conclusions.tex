We presented for the first time in the literature an algorithm to find an optimal deterministic policy that minimizes the maximum regret of a Markov Decision Processes (MDP) with imprecise rewards.
The proposed algorithm consists of a branch-and-bound that uses Benders decomposition as bounding procedure. In addition to a basic implementation, we propose a cut-and-branch implementation that turns out to reduce the overall computing time on average by $50\%$.  
We motivate the use of deterministic over stochastic policies by showing theoretically that basic rounding procedures find deterministic policies far from the optimal. Secondly, we show that the additional computational effort of computing the optimal deterministic policy in comparison to the one needed to compute the optimal stochastic policy is acceptable (approximately one order of magnitude slower). 
%We hope that this manuscript motivates the scientific community to investigate more the development of algorithm for deterministic solutions in the context of imprecise MDPs.
\paragraph{Future perspective: improving the readability for the user by allowing a limited number of selected actions.}
A stochastic policy can potentially recommend many actions for each state. It is harder for a user, especially under time pressure, to select an action among the array of available actions suggested by the stochastic policy. 
%
One of the motivations of looking for a deterministic policy is precisely to have a solution that is easy to interpret.

The requirement of having a deterministic policy can be relaxed by allowing multiple possible actions per state but, at the same time, still limiting the total number of choices in each state.
%
In figure~\ref{fig:impact_mixed_policy} we show how the value of the optimal policy decreases when the maximum number of allowed actions increases. Each curve represents a \texttt{Random-lim} MDP with $6$ actions and $4$ to $7$ states. On the horizontal axis we have the maximum number of actions allowed in the optimal policy; a value equal to $1$ corresponds to the case of a deterministic policy and the value equal to $6$ corresponds to the stochastic policy. Vertically we show the difference between the value of the optimal policy for a given number of allowed actions and the optimal stochastic policy. We normalize the results by dividing them by the maximum gap (for this reason we always have a gap of $100\%$ for the deterministic policy and a gap of $0\%$ for the stochastic policy). We report the results for \texttt{Random-lim} MDPs while we experimentally observed that the behavior is the same for all the studied instances. We were able to obtain such results by a generalization of our branch-and-bound algorithms. 



It is important to notice that allowing to have a maximum of two possible actions per state leads to a policy with a maximum regret significantly closer to the optimal stochastic policy. These results suggest that an interesting trade-off could be to allow a maximum of two or three states, in this way the policy remains easy to interpret for the user and the value of the maximum regret is only marginally worse than the one of the optimal stochastic policy.
If a future theoretical study confirms our observations for certain categories of applications, then it will open a concrete perspective to the use of ``determinization'' for these applications leaving reasonable choices to the user while keeping a quality of the policy close to the optimal.