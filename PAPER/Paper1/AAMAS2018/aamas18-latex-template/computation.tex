\section{Experimental results}\label{sec:experiments}

In this section, we provide an experimental evaluation of our algorithms based on \ET{three} classes of test instances: %More precisely, we test our approach on the following sets of IRMDPs: 
random MDPs with unlimited connections, random MDPs with limited connections and diamond MPDs.  
The aim of this section is twofold: we first investigate how in practice the optimal deterministic policy is different from the determinized policy obtained from the optimal stochastic policy. Secondly, we show how the new cut-and-branch version of the algorithm helps to solve faster the instances considered.

% (1) Random MDPs (\texttt{Random}), (2)  Random MDPs with limited connections (\texttt{Random-lim}) and (3) Diamond MDPs (\texttt{Diamond}) .
%\item Grid MDPs (\texttt{Grid}).

\subsection{Instance description}
\paragraph{Random MDPs, unlimited connections (\texttt{Random-unlim})}
A random-unlim MDP is defined by a given number of states $|S|$ and actions $|A|$. The rewards are bounded between two real random values uniformly selected in the intervall $[-1.0,1.0]$. 
%The transition function has the following properties: from any state $s$ we restrict transitions to reach $\lceil \log_2(n) \rceil$ states. 
For each pair of $(s, a)$ we draw reachable states based on uniform distribution over the set of states. For drawn states, the transition probabilities are formed based on Gaussian distribution. The initial state distribution $\beta$ is uniform and we choose a discount factor $\gamma = 0.95$. 
\paragraph{Random MDPs, limited connections (\texttt{Random-unlim})}
A random-lim MDP is a random MDP where the number of destination states that can be reached with a given action are limited in comparison with random-unlimit MDPs. A random-lim MDP is identified by its number of states $|S|$ and a fixed number of reachable states $n$. The rewards are bounded between two real random values uniformly selected in the intervall $[-1.0,1.0]$.
The transition function has the following properties: from any state $s$ the number of reachable states is equal to $n$, uniformly selected over the set of states.
The number of actions depends on the value of $n$. First, we define $n$ actions, where each of them reaches only a single state among the allowed ones. Secondly, each of the remaining actions reaches two states. Therefore, we have a total of $n+\frac{n(n-1)}{2}$ actions. The initial state distribution $\beta$ is uniform and we choose a discount factor $\gamma = 0.95$.
\paragraph{Diamond MDPs (\texttt{Diamond})}
This class of MDPs has been introduced for the first time in Benavent and Zanuttini~\cite{benavent2018}. 
In this family of problems, the reward of a few states suffices to generate a lot of uncertainties about the optimal policy. This IRMDP is an interesting set of instances to test our proposed algorithm. 

This class of MDPs has a diamond structure, with one top and one bottom state (playing the role of start and terminal of the MDP), one intermediate layer of states, containing all the uncertainties on rewards, plus two intermediate layers between the extreme states and the intermediate layer. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=8cm]{images/diamond.png}
\end{center}
\caption{Diamond MDP: actions $a_0$ (left) and $a_1, a_2$ (right) (Figure taken from \cite{benavent2018}).}
\label{fig:diamond}
\end{figure}


%From a point of viex of the parameters, the authors proposed to have that action $a_0$ would reach each child with a probability of $0.5$.
The diamond MDP structure is given in Figure~\ref{fig:diamond}. Action $a_0$ has probability $0.5$ to reach each child node.  
On the other hand $a_1$ (resp. $a_2$) has a probability of $p= 0.3$ (resp. $1-p$) to reach the left (resp. right) child node and to reach its parent otherwise.
The imprecise values of the rewards for the middle layer are $[-600,600]$, while the one of the bottom node is $[600,1000]$.

We propose a generalization of this family of MDP by testing a range of parameters for the probability $p \in \{0.05,0.10,\dots,0.40,0.45\}$. %We also introduce an additional intermediate layer, between the extreme states and the middle layer. In this way we have, in addition to $10$-states MDPs (callend one-level diamond MDPs) also  22-states MDPs (called two-level diamond MDPs). 


\subsection{Comparison with the determinised policy}

For a given MDP, let $MR(f^{\hat{\pi}}, \mathcal{R})$ be the maximum regret of the rounding deterministic policy and $MR(f^{\pi^*}, \mathcal{R})$  be the maximum regret of the optimal deterministic policy. We define the Value Ratio of such MDPs as: $VR = \dfrac{MR(f^{\hat{\pi}}, \mathcal{R})}{MR(f^{\pi^*}, \mathcal{R})}\;$.
%Moreover, let $\hat{T}$ (respectively $T^*$) be the computing time necessary to calculate the rounding (respectively optimal) deterministic policy, we define the Time Ratio as: $TR=\dfrac{T^*}{\hat{T}}\;$. \ET{(Practically speaking, $\hat T$ reduces to the time needed for computing the stochastic policy)}
The VT gives an idea about how far is the determinised policy from the optimal policy. 
%\input{tab_random.tex}
\input{tab_random_slim.tex}  
\input{tab_random_trident_slim.tex}  
\input{tab_random_diamond_slim.tex}  


In Tables~\ref{tab:random_slim},~\ref{tab:random_trident_slim} and~\ref{tab:diamond_slim} we present the results concerning the value Rations for the different classes of instances considered. In case of multiple isntances for compbination of settings, we present the average and the maximum value.
We notice that the \texttt{Random-lim} and \texttt{Diamond} MDPs are the ones with an highest VR in general. The relative small value of VR for the \texttt{Random-unlim} MDPs is probably due to the fact that they do not present a special structure. For such instances it is more unlikely to have extreme configurations like the one showed in Section~\ref{sec:comparison}. 

It is also interesting to notice that the maximum values are quite high for almost all the combinations of parameters considered. the maximum 

 This implies that chosing to determinise my reveal as a unsafe choice, specially considering that there is no way to check how far a determinisied policy is from the optimal.  
% $70\%$ of the times the optimal deterministic policy differs from the rounding deterministic policy, while the maximum regret of the rounding deterministic policy is $6\%$ worse than the optimal deterministic policy. This moderate gap is probably due to the fact that random MDPs do not present a special structure. For such instances it is more unlikely to have extreme configurations like the one showed in Section~\ref{sec:comparison}.

%Calculating the optimal deterministic policy is one order of magnitude slower than computing the rounding deterministic policy. On the other hand, the cut-and-branch version of the algorithm is almost two times faster than the basic version.  



%\subsection{Random MDPs}\label{random_MDP}
%\paragraph{\texttt{Random-unlim}}
%\paragraph{Analysis of the results}
%In Table~\ref{tab:random} we present the results concerning the performances of our algorithm on random MDP with $|S| \in \{5,10,15\}$ and $|A| \in \{2, 3, 4, 5, 10\}$. For each combination of states and actions, we provide the average results over $10$ different simulations. The first two columns report the Value Ratio and the Time Ratio (\texttt{VR} and \texttt{TR}). The column \texttt{\% diff} shows the percentage of cases where the optimal policy is different from the rounding policy. The final two columns show the computing time of the baseline branch-and-bound algorithm (\texttt{Base}) and the improved version (\texttt{C\&B}), presented in Section~\ref{sec:bb}.

We notice that in average, $70\%$ of the times the optimal deterministic policy differs from the rounding deterministic policy, while the maximum regret of the rounding deterministic policy is $6\%$ worse than the optimal deterministic policy. This moderate gap is probably due to the fact that random MDPs do not present a special structure. For such instances it is more unlikely to have extreme configurations like the one showed in Section~\ref{sec:comparison}.
Calculating the optimal deterministic policy is one order of magnitude slower than computing the rounding deterministic policy. On the other hand, the cut-and-branch version of the algorithm is almost two times faster than the basic version.  
%Finally, it is interesting to notice that in the basic implementation of the algorithm, the number of additional Benders cuts added during the branch-and-bound is significantly low (on average no more than $2$ cuts per node). This explains why the cut-and-branch version of the branch-and-bound algorithm performs better than the standard version.
 
\paragraph{\texttt{Diamond}}
In Table~\ref{tab:diamond}, we show how the Time Ratio and the Value ratio change with the increase of $p$.
It is clear that for Diamond MDPs the situation is different than for Random MDPs. In this case, the max regret of the rounding deterministic policy is $20\%$ worse than the one of the optimal deterministic policy. Moreover, the computing time for the optimal deterministic policy is less than one order of magnitude lower than the one needed by the rounding deterministic policy. These results show how, in presence of a specific structure, the difference between $MR(f^{\hat{\pi}}, \mathcal{R})$ and $MR(f^{\pi^*}, \mathcal{R})$ increases significantly.






\subsection{Impact of the cut-and-branch improvement}


%%%%%%%%%%%%%%%%%
\begin{figure}[]
	\begin{center}
    \includegraphics[scale=0.45]{GNUPLOT/output_random.png}
	\end{center}
	\caption{Impact of cut-and-branch, \texttt{Random-unlim} MDPs.}
	\label{fig:impact_random} 
\end{figure}
%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%
\begin{figure}[]
	\begin{center}
    \includegraphics[scale=0.45]{GNUPLOT/output_trident.png}
	\end{center}
	\caption{Impact of cut-and-branch, \texttt{Random-lim} MDPs.}
	\label{fig:impact_trident} 
\end{figure}
%%%%%%%%%%%%%%%%%