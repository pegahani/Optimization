%%================================================================

% this part is deleted in EGC version
%\textbf{Literature Review.}  
Typically, in MDPs the reward functions are estimated either from observations or external sources. Mannor et al. \shortcite{Mannor2007} demonstrate that the policy found via an optimisation process under the MDPs with exact numerical parameters, sometimes can be much worse than the anticipated policy under bounded and imprecise reward values. This motivates using MDPs that account for this ambiguity in model parameters. To cope with this problem, first the \textit{MDP with imprecise rewards (IRMDP)} should be modelled mathematically. There exists several models in the literature including symbolic-based rewards approaches \cite{Furnkranz2012,Weng2012} and numerical-based rewards approaches \cite{bell1982,Regan2009,Xu2009}. We focus on group of approaches taking decision-theoretic point of view, and consider a set of MDPs with different reward functions --modeling under uncertainty-- namely \textit{Imprecise Reward MPDs (IRMDPs)}. 
% this part is deleted in EGC version

%In order to find the optimal policy under uncertainty, robust solutions typically provides guaranties on the worst case performance. 
One common approach in computing a robust solution is the \textit{maximin} method, that computes a policy maximising the value with respect to the worst-case scenario \cite{GIVAN2000,Iyengar2005,mastin2012,Nilim2005}. \textit{maximin} policies are conservative naturally \cite{Delage2007}, thus \textit{minimax regret} approach \cite{Regan2009,Xu2009} has been introduced as an approach to cope with this issue. The minimax robustness can be considered as a game between two adversaries, one finds a policy with maximum values while the adversary chooses an instantiation of the reward functions that minimise the expected value. There are some recent works that propose some techniques for dependent uncertainties in MDPs \cite{Mannor2012,Wiesemann2013}. In this paper, the uncertain reward functions are independent from each other. 

%In \textit{minimax} regret criterion the goal is to find a policy that has the least value of maximum regret over all instantiations of rewards. 
Several methods in the past have only focused on computing  \textit{optimal stochastic policies} for IRMDPs \ET{optimisation approaches} \cite{Regan2009,Regan2010,Xu2009}. %, i.e. a policy proposing several actions associated with probabilities for a state in IRMDP. 
\ET{\shortcite{Ahmed2017} proposed a mixed integer linear program for computing the optimal deterministic policies when a set of samples from uncertain rewards is given.  While we propose an approach receiving the set of whole uncertain rewards without any sampling. }
To the best of our knowledge, there is no work that handle deterministic policy computation on MDPs under uncertainties \ET{using exact optimisation approaches }. The existing works on deterministic policies computation deal usually with MDPs with precise parameters \cite{Dolgov2005,Montufar2015}. %In this paper we introduce an approach for computing the closest deterministic policy with the minimum value on maximum regret over all instantiations of rewards. 